# app/utils/search.py
import scrapetube
from pytube import YouTube
from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, NoTranscriptFound, NoTranscriptAvailable
import logging
from config import Config
import threading
import requests
import os
import re
import google.generativeai as genai

logger = logging.getLogger(__name__)

# Configurer Gemini API
genai.configure(api_key=Config.GEMINI_API_KEY)
model = genai.GenerativeModel("gemini-1.5-flash-8b")

# Initialiser une session persistante
session = requests.Session()

def generate_search_queries(user_input, num_queries=Config.NUM_SEARCH_QUERIES):
    prompt = (
        f"Based on the user's query: \"{user_input}\", generate up to {num_queries} concise and relevant YouTube search queries. "
        "Each query should be 2 to 4 words long, highly relevant to the user's interest, and suitable for a YouTube search. "
        "Provide each query on a separate line without numbering or additional text."
    )
    try:
        logger.info("Generating search queries with Gemini API.")
        response = model.generate_content(prompt)
        generated_text = response.text.strip()

        # Extraire les requêtes de recherche
        search_queries = [line.strip() for line in generated_text.split('\n') if line.strip()]
        if search_queries:
            logger.info(f"Generated search queries: {search_queries}")
            return {'type': 'queries', 'content': search_queries[:num_queries]}
        else:
            logger.warning("No search queries generated by Gemini.")
            return {'type': 'error', 'content': []}
    except Exception as e:
        logger.error(f"Error generating search queries: {e}")
        return {'type': 'error', 'content': []}

def search_videos(query, max_results=Config.MAX_RESULTS_PER_QUERY):
    try:
        logger.info(f"Searching videos for query: '{query}'")
        # Utiliser Scrapetube pour rechercher des vidéos
        videos = scrapetube.get_search(query, limit=max_results)
        video_ids_titles = []
        for video in videos:
            video_id = video['videoId']
            title_dict = video.get('title', {})
            title = ''
            if 'runs' in title_dict and len(title_dict['runs']) > 0:
                title = ''.join([run.get('text', '') for run in title_dict['runs']])
            elif 'simpleText' in title_dict:
                title = title_dict['simpleText']
            video_ids_titles.append((video_id, title))
        logger.info(f"Found {len(video_ids_titles)} videos for '{query}'")
        return video_ids_titles
    except Exception as e:
        logger.error(f"Error searching videos for query '{query}': {e}")
        return []

def extract_video_data_worker(video_id, results):
    data = extract_video_data(video_id)
    if data:
        results.append(data)

def extract_video_data(video_id):
    try:
        logger.info(f"Extracting metadata for video ID: {video_id}")
        yt = YouTube(f"https://www.youtube.com/watch?v={video_id}")
        title = yt.title if yt.title else "Title unavailable"
        description = yt.description if yt.description else "Description unavailable"
        thumbnail_url = yt.thumbnail_url if yt.thumbnail_url else ""
        length = yt.length  # Durée en secondes
        publish_date = yt.publish_date.strftime('%Y-%m-%d') if yt.publish_date else "Publication date unavailable"

        # Obtenir les transcriptions
        try:
            transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)
            # Préférer les transcriptions manuelles en anglais, sinon générées
            try:
                transcript = transcript_list.find_manually_created_transcript(['en'])
                transcript_type = 'manual'
                logger.info(f"Manual subtitles found in 'en' for video {video_id}.")
            except NoTranscriptFound:
                transcript = transcript_list.find_generated_transcript(['en'])
                transcript_type = 'generated'
                logger.info(f"Generated subtitles found in 'en' for video {video_id}.")

            transcript_data = transcript.fetch()
            # Convertir la transcription en texte continu
            subtitles_text = ' '.join([entry['text'] for entry in transcript_data])
            # Limiter la longueur de la transcription
            cleaned_subtitles = subtitles_text[:Config.MAX_SUBTITLE_LENGTH]
        except (TranscriptsDisabled, NoTranscriptFound, NoTranscriptAvailable) as e:
            logger.warning(f"Transcript not available for video ID: {video_id}. Reason: {e}")
            cleaned_subtitles = ""

        logger.info(f"Successfully extracted data for video {video_id}")

        return {
            'video_id': video_id,
            'title': title,
            'description': description,
            'thumbnail_url': thumbnail_url,
            'length': length,
            'publish_date': publish_date,
            'transcript': cleaned_subtitles,
            'status': 'data_extracted'
        }
    except YouTube.exceptions.VideoUnavailable:
        logger.error(f"Video unavailable for ID: {video_id}")
        return None  # Retourner None pour les vidéos indisponibles
    except YouTube.exceptions.RegexMatchError:
        logger.error(f"Regex match error for video ID: {video_id}")
        return None
    except Exception as e:
        logger.error(f"Unexpected error extracting data for video {video_id}: {e}")
        return None

def process_query(query):
    video_ids_titles = search_videos(query)
    results = []
    threads = []
    
    for video_id, _ in video_ids_titles:
        if len(threads) >= Config.MAX_THREADS:
            # Attendre que les threads se terminent
            for t in threads:
                t.join()
            threads = []
        t = threading.Thread(target=extract_video_data_worker, args=(video_id, results))
        t.start()
        threads.append(t)

    # Attendre que les threads restants se terminent
    for t in threads:
        t.join()

    return results
